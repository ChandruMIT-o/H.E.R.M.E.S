{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENAI_API_KEY = \"AIzaSyDWki8PZMr6rotpuelz6xdXhw0h63YBYeQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "import PIL.Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HELPERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Read Text .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_text(filename):\n",
    "    \"\"\"Reads a text file and returns its content as a string.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Read a json file as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_as_string(file_path):\n",
    "    \"\"\"Reads a JSON file and returns its content as a string.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.dumps(json.load(file), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Extracts JSON content from a string enclosed in json ``` ```  and returns it as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_string(text):\n",
    "    \"\"\"Extracts JSON content from a string enclosed in json``` ``` and returns it as a dictionary.\"\"\"\n",
    "    match = re.search(r'```json(.*?)```', text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1).strip())\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"Invalid JSON format inside the markers.\")\n",
    "    else:\n",
    "        raise ValueError(\"No JSON content found in the provided string.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Extracts JSON content from an image for Question Paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_with_gemini_to_json(image_path):\n",
    "    try:\n",
    "        # Load the image\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            img = PIL.Image.open(io.BytesIO(img_file.read()))\n",
    "\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "        qp_json_format = read_json_as_string('question_paper.json')\n",
    "\n",
    "        prompt = \"Extract and return the text from this image into a carefully formatted question paper which follows this json format. (Your task is to return the json content between json``` content ```), also understand the rubrics for the given question paper and replace the sample rubric in the json given. In case of either or questions. Assume Part A, B, C and so on from the question paper. max_attempts in rubric means, how many questions are supposed to be written per section for maximum marks.\" + qp_json_format\n",
    "        response = model.generate_content([prompt, img])\n",
    "\n",
    "        return response.text if response.text else \"No text detected\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Extracts JSON content from text for Question Paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qp_string_with_gemini_to_json(qp_string):\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "        qp_json_format = read_json_as_string('question_paper.json')\n",
    "\n",
    "        prompt = \"This is content extracted from a question paper and return this text into a carefully formatted question paper which follows this json format. (Your task is to return the json content between json``` content ```), also understand the rubrics for the given question paper and replace the sample rubric in the json given. In case of either or questions. Assume Part A, B, C and so on from the question paper. max_attempts in rubric means, how many questions are supposed to be written per section for maximum marks.\" + qp_json_format\n",
    "        response = model.generate_content([prompt, qp_string])\n",
    "\n",
    "        return response.text if response.text else \"No text detected\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Extracts Answer Key from an Image and adds it to the extracted Json formatted question paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_answer_key_to_qp_json(image_path, qp_json_format):\n",
    "    try:\n",
    "        # Load the image\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            img = PIL.Image.open(io.BytesIO(img_file.read()))\n",
    "\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "        prompt = \"Extract and return the answers from this image into this carefully formatted question paper which follows this json format. (Your task is to return the json content between json``` content ```) along with the extracted answers, also understand the rubrics for the given question paper while extracting the answers from the image.\" + qp_json_format\n",
    "        response = model.generate_content([prompt, img])\n",
    "\n",
    "        return response.text if response.text else \"No text detected\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Extracts Answer Key from text and adds it to the extracted Json formatted question paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_key_string_with_gemini_to_json(ans_key_string, qp_json_format):\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "        prompt = \"This is content extracted from an answer sheet, return this text into a carefully formatted question paper which follows this json format. (Your task is to return the json content between json``` content ```), along with the extracted answers, also understand the rubrics for the given question paper while extracting the answers from this text.\" + qp_json_format\n",
    "        response = model.generate_content([prompt, ans_key_string])\n",
    "\n",
    "        return response.text if response.text else \"No text detected\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Extract and Evaluate an Answer Sheet from an image and creates a student answer sheet instance (JSON formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_evaluate_answer_sheet_with_key(answer_sheet_img_path:str, qp_with_key:str, answer_format:str):\n",
    "    try:\n",
    "        # Load the image\n",
    "        with open(answer_sheet_img_path, \"rb\") as img_file:\n",
    "            img = PIL.Image.open(io.BytesIO(img_file.read()))\n",
    "\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "        prompt = \"Extract and return the answers from this image into this carefully formatted answer sheet which follows this json format. (Your task is to return the json content between json``` content ```) along with the extracted answers, also understand the rubrics for the given question paper while extracting the answers from the image. Only answers with appropriate question id should be there in the json output. Evaluate the answers based on the question paper and mark the answers with an explanation as required in the answer json format. Tally the marks and fill the answer_sheet_format properly. Only return the answer_sheet_format with answers extracted from the image along with proper evaluation.\" + f\"QUESTION_PAPER: {qp_with_key}\" + \"ANSWER_SHEET_FORMAT: {answer_format}\"\n",
    "        response = model.generate_content([prompt, img])\n",
    "\n",
    "        return response.text if response.text else \"No text detected\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Question Paper and Rubrics Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_title': 'Comprehensive Exam Paper',\n",
       " 'subject': 'Computer Vision',\n",
       " 'subject-id': 'XXXX',\n",
       " 'exam_date': '[Month/Year]',\n",
       " 'duration': '120',\n",
       " 'instructions': 'Answer all questions in PART A. Answer any two questions from PART B and any one question from PART C. Use diagrams where necessary and provide clear, concise answers.',\n",
       " 'questions': [{'id': 'Q1',\n",
       "   'type': 'short_answer',\n",
       "   'question_text': 'Define image convolution and its significance in computer vision.',\n",
       "   'diagrams': [],\n",
       "   'marks': 2,\n",
       "   'options': [],\n",
       "   'answer': '',\n",
       "   'metadata': {'difficulty': 'medium',\n",
       "    'topics': ['image processing', 'convolution']},\n",
       "   'evaluation-prompt': \"Assess the student's understanding of image convolution and its applications in computer vision.  Consider accuracy and completeness of the definition and significance.\",\n",
       "   'related_url': []},\n",
       "  {'id': 'Q2',\n",
       "   'type': 'short_answer',\n",
       "   'question_text': 'What is the difference between edge detection and corner detection?',\n",
       "   'diagrams': [],\n",
       "   'marks': 2,\n",
       "   'options': [],\n",
       "   'answer': '',\n",
       "   'metadata': {'difficulty': 'medium',\n",
       "    'topics': ['feature detection', 'edge detection', 'corner detection']},\n",
       "   'evaluation-prompt': \"Evaluate the student's ability to differentiate between edge and corner detection techniques.  Look for clarity and accuracy in explaining the differences.\",\n",
       "   'related_url': []},\n",
       "  {'id': 'Q3',\n",
       "   'type': 'short_answer',\n",
       "   'question_text': 'Explain the concept of homography in image transformations.',\n",
       "   'diagrams': [],\n",
       "   'marks': 2,\n",
       "   'options': [],\n",
       "   'answer': '',\n",
       "   'metadata': {'difficulty': 'medium',\n",
       "    'topics': ['image transformations', 'homography']},\n",
       "   'evaluation-prompt': \"Assess the student's understanding of homography and its role in image transformations.  A clear and concise explanation is required.\",\n",
       "   'related_url': []},\n",
       "  {'id': 'Q4',\n",
       "   'type': 'short_answer',\n",
       "   'question_text': 'What are the challenges in object recognition using deep learning?',\n",
       "   'diagrams': [],\n",
       "   'marks': 2,\n",
       "   'options': [],\n",
       "   'answer': '',\n",
       "   'metadata': {'difficulty': 'hard',\n",
       "    'topics': ['object recognition', 'deep learning', 'challenges']},\n",
       "   'evaluation-prompt': \"Evaluate the student's awareness of the complexities involved in object recognition using deep learning methods. The answer should mention multiple challenges.\",\n",
       "   'related_url': []},\n",
       "  {'id': 'Q5',\n",
       "   'type': 'short_answer',\n",
       "   'question_text': 'Describe the role of feature extraction in computer vision.',\n",
       "   'diagrams': [],\n",
       "   'marks': 2,\n",
       "   'options': [],\n",
       "   'answer': '',\n",
       "   'metadata': {'difficulty': 'medium',\n",
       "    'topics': ['feature extraction', 'computer vision']},\n",
       "   'evaluation-prompt': \"Assess the student's understanding of the importance of feature extraction in computer vision tasks.  The answer should be concise and accurate.\",\n",
       "   'related_url': []},\n",
       "  {'id': 'Q6',\n",
       "   'type': 'either_or',\n",
       "   'question_text': 'Either (a) Explain the various types of image segmentation techniques with examples. Or (b) Describe the working of the Harris Corner Detector with necessary mathematical derivations.',\n",
       "   'diagrams': [],\n",
       "   'marks': 13,\n",
       "   'options': [{'option_id': 'A',\n",
       "     'text': 'Explain the various types of image segmentation techniques with examples.'},\n",
       "    {'option_id': 'B',\n",
       "     'text': 'Describe the working of the Harris Corner Detector with necessary mathematical derivations.'}],\n",
       "   'answer': '',\n",
       "   'metadata': {'difficulty': 'hard',\n",
       "    'topics': ['image segmentation', 'Harris Corner Detector']},\n",
       "   'evaluation-prompt': \"For (a), evaluate the student's understanding of different image segmentation techniques and their applications.  For (b), assess their ability to explain the Harris Corner Detector algorithm and the associated mathematical concepts.\",\n",
       "   'related_url': []},\n",
       "  {'id': 'Q7',\n",
       "   'type': 'either_or',\n",
       "   'question_text': 'Either (a) Discuss the structure and working of a Convolutional Neural Network (CNN). Illustrate with a suitable architecture. Or (b) Explain the role of Optical Flow in motion estimation. Derive the Lucas-Kanade Optical Flow equation.',\n",
       "   'diagrams': [],\n",
       "   'marks': 13,\n",
       "   'options': [{'option_id': 'A',\n",
       "     'text': 'Discuss the structure and working of a Convolutional Neural Network (CNN). Illustrate with a suitable architecture.'},\n",
       "    {'option_id': 'B',\n",
       "     'text': 'Explain the role of Optical Flow in motion estimation. Derive the Lucas-Kanade Optical Flow equation.'}],\n",
       "   'answer': '',\n",
       "   'metadata': {'difficulty': 'hard',\n",
       "    'topics': ['CNN', 'Optical Flow', 'motion estimation']},\n",
       "   'evaluation-prompt': \"For (a), evaluate the student's understanding of CNN architecture and functionality.  For (b), assess their ability to explain optical flow and derive the Lucas-Kanade equation.\",\n",
       "   'related_url': []},\n",
       "  {'id': 'Q8',\n",
       "   'type': 'either_or',\n",
       "   'question_text': 'Either (a) Explain in detail the different methods for object detection in images and videos. Compare traditional approaches with deep learning-based methods. Or (b) What is feature matching in computer vision? Discuss different feature matching techniques and their applications.',\n",
       "   'diagrams': [],\n",
       "   'marks': 14,\n",
       "   'options': [{'option_id': 'A',\n",
       "     'text': 'Explain in detail the different methods for object detection in images and videos. Compare traditional approaches with deep learning-based methods.'},\n",
       "    {'option_id': 'B',\n",
       "     'text': 'What is feature matching in computer vision? Discuss different feature matching techniques and their applications.'}],\n",
       "   'answer': '',\n",
       "   'metadata': {'difficulty': 'hard',\n",
       "    'topics': ['object detection', 'feature matching']},\n",
       "   'evaluation-prompt': 'For (a), evaluate the comprehensive understanding of object detection methods and their comparison. For (b), assess the knowledge of feature matching techniques and their applications in computer vision.',\n",
       "   'related_url': []}],\n",
       " 'metadata': {'created_by': '[Exam Committee Name]',\n",
       "  'version': '1.0',\n",
       "  'rubric_exam_name': {'A': {'questions': ['Q1', 'Q2', 'Q3', 'Q4', 'Q5'],\n",
       "    'max_attempts': 5,\n",
       "    'marks_per_question': 2},\n",
       "   'B': {'questions': ['Q6', 'Q7'],\n",
       "    'max_attempts': 2,\n",
       "    'marks_per_question': 13},\n",
       "   'C': {'questions': ['Q8'], 'max_attempts': 1, 'marks_per_question': 14}},\n",
       "  'total_points': 50,\n",
       "  'pass_threshold': 15}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_path = \"qp1.txt\"\n",
    "qp_string = read_from_text(\"qp1.txt\")\n",
    "extracted_text = qp_string_with_gemini_to_json(qp_string)\n",
    "\n",
    "json_data = extract_json_from_string(extracted_text)\n",
    "\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"qp1.png\"\n",
    "extracted_text = ocr_with_gemini_to_json(image_path)\n",
    "print(\"Extracted Text:\\n\", extracted_text)\n",
    "json_data = extract_json_from_string(extracted_text)\n",
    "json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Answer Key extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"paper_title\": \"Comprehensive Exam Paper\",\\n  \"subject\": \"Computer Vision\",\\n  \"subject-id\": \"XXXX\",\\n  \"exam_date\": \"[Month/Year]\",\\n  \"duration\": 120,\\n  \"instructions\": \"Answer all questions as directed. Use diagrams where necessary and provide clear, concise answers.\",\\n  \"questions\": [\\n    {\\n      \"id\": \"Q1\",\\n      \"type\": \"short_answer\",\\n      \"question_text\": \"Define image convolution and its significance in computer vision.\",\\n      \"diagrams\": [],\\n      \"marks\": 2,\\n      \"options\": [],\\n      \"answer\": \"Image convolution is a mathematical operation on two functions (in this case, the image and a kernel/filter) that produces a modified image emphasizing or suppressing certain features. It is used for smoothing, sharpening, edge detection, and feature extraction, making it fundamental for preprocessing and analyzing images in computer vision.\",\\n      \"metadata\": {\\n        \"difficulty\": \"easy\",\\n        \"topics\": [\\n          \"image processing\",\\n          \"convolution\"\\n        ]\\n      },\\n      \"evaluation-prompt\": \"Assess the correctness and completeness of the definition and explanation of significance.\",\\n      \"related_url\": []\\n    },\\n    {\\n      \"id\": \"Q2\",\\n      \"type\": \"short_answer\",\\n      \"question_text\": \"What is the difference between edge detection and corner detection?\",\\n      \"diagrams\": [],\\n      \"marks\": 2,\\n      \"options\": [],\\n      \"answer\": \"Edge detection identifies boundaries within images where there is a sharp change in intensity, typically indicating object borders. In contrast, corner detection finds points where edges meet or where there is a high curvature, which are useful as reliable feature points for tracking and matching.\",\\n      \"metadata\": {\\n        \"difficulty\": \"medium\",\\n        \"topics\": [\\n          \"feature detection\",\\n          \"edge detection\",\\n          \"corner detection\"\\n        ]\\n      },\\n      \"evaluation-prompt\": \"Evaluate the clarity and accuracy of the distinctions drawn between edge and corner detection.\",\\n      \"related_url\": []\\n    },\\n    {\\n      \"id\": \"Q3\",\\n      \"type\": \"short_answer\",\\n      \"question_text\": \"Explain the concept of homography in image transformations.\",\\n      \"diagrams\": [],\\n      \"marks\": 2,\\n      \"options\": [],\\n      \"answer\": \"Homography is a transformation that maps points from one plane to another using a 3x3 matrix. It is used to correct perspective distortions, align images taken from different viewpoints, and is fundamental in tasks like image stitching and augmented reality.\",\\n      \"metadata\": {\\n        \"difficulty\": \"medium\",\\n        \"topics\": [\\n          \"image transformation\",\\n          \"homography\"\\n        ]\\n      },\\n      \"evaluation-prompt\": \"Assess the understanding of homography and its role in image transformations.\",\\n      \"related_url\": []\\n    },\\n    {\\n      \"id\": \"Q4\",\\n      \"type\": \"short_answer\",\\n      \"question_text\": \"What are the challenges in object recognition using deep learning?\",\\n      \"diagrams\": [],\\n      \"marks\": 2,\\n      \"options\": [],\\n      \"answer\": \"Challenges include: Data Requirements: Deep networks require large, diverse, and well-labeled datasets. Overfitting: With limited data, models may overfit and not generalize well. Computational Resources: Training deep networks is computationally intensive. Variability: Handling variations in lighting, occlusion, and pose can be difficult. Interpretability: Deep models are often seen as \\\\\"black boxes,\\\\\" making it hard to explain their decisions.\",\\n      \"metadata\": {\\n        \"difficulty\": \"hard\",\\n        \"topics\": [\\n          \"object recognition\",\\n          \"deep learning\",\\n          \"challenges\"\\n        ]\\n      },\\n      \"evaluation-prompt\": \"Evaluate the identification and explanation of significant challenges.\",\\n      \"related_url\": []\\n    },\\n    {\\n      \"id\": \"Q5\",\\n      \"type\": \"short_answer\",\\n      \"question_text\": \"Describe the role of feature extraction in computer vision.\",\\n      \"diagrams\": [],\\n      \"marks\": 2,\\n      \"options\": [],\\n      \"answer\": \"Feature extraction involves identifying and isolating important pieces of information (features) from raw images such as edges, corners, textures, or shapes. These features form the basis for tasks like object recognition, image matching, and classification, reducing data complexity and improving processing efficiency.\",\\n      \"metadata\": {\\n        \"difficulty\": \"medium\",\\n        \"topics\": [\\n          \"feature extraction\",\\n          \"computer vision\"\\n        ]\\n      },\\n      \"evaluation-prompt\": \"Assess the understanding of feature extraction\\'s importance in computer vision.\",\\n      \"related_url\": []\\n    },\\n    {\\n      \"id\": \"Q6\",\\n      \"type\": \"either_or\",\\n      \"question_text\": \"Either (a) Explain the various types of image segmentation techniques with examples. Or (b) Describe the working of the Harris Corner Detector with necessary mathematical derivations.\",\\n      \"diagrams\": [],\\n      \"marks\": 13,\\n      \"options\": [\\n        {\\n          \"option_id\": \"a\",\\n          \"text\": \"Explain the various types of image segmentation techniques with examples.\"\\n        },\\n        {\\n          \"option_id\": \"b\",\\n          \"text\": \"Describe the working of the Harris Corner Detector with necessary mathematical derivations.\"\\n        }\\n      ],\\n      \"answer\": \"Image segmentation divides an image into meaningful regions. Techniques include: Thresholding: Separates regions by intensity values (e.g., separating foreground from background). Edge-based Segmentation: Detects object boundaries using gradient information. Region-based Segmentation: Groups pixels with similar properties, such as in region growing or splitting and merging. Clustering Methods: Algorithms like k-means cluster pixels based on color or texture similarities. Deep Learning Approaches: Methods such as Fully Convolutional Networks (FCN) and U-Net learn to segment images with high accuracy in complex scenes. In practice, each method is chosen based on the image characteristics and the specific application requirements.\",\\n      \"metadata\": {\\n        \"difficulty\": \"hard\",\\n        \"topics\": [\\n          \"image segmentation\",\\n          \"harris corner detection\"\\n        ]\\n      },\\n      \"evaluation-prompt\": \"Evaluate the completeness and accuracy of the explanation or derivation, including mathematical rigor where applicable.\",\\n      \"related_url\": []\\n    },\\n    {\\n      \"id\": \"Q7\",\\n      \"type\": \"either_or\",\\n      \"question_text\": \"Either (a) Discuss the structure and working of a Convolutional Neural Network (CNN). Illustrate with a suitable architecture. Or (b) Explain the role of Optical Flow in motion estimation. Derive the Lucas-Kanade Optical Flow equation.\",\\n      \"diagrams\": [],\\n      \"marks\": 13,\\n      \"options\": [\\n        {\\n          \"option_id\": \"a\",\\n          \"text\": \"Discuss the structure and working of a Convolutional Neural Network (CNN). Illustrate with a suitable architecture.\"\\n        },\\n        {\\n          \"option_id\": \"b\",\\n          \"text\": \"Explain the role of Optical Flow in motion estimation. Derive the Lucas-Kanade Optical Flow equation.\"\\n        }\\n      ],\\n      \"answer\": \"A Convolutional Neural Network (CNN) is composed of several types of layers designed to automatically and adaptively learn spatial hierarchies of features such as edges and textures. Convolutional Layers: Apply filters to extract features such as edges and textures. Activation Layers: Introduce non-linearity using functions like ReLU. Pooling Layers: Reduce the spatial dimensions, making the representation more compact and robust to slight translations (e.g., max pooling). Fully Connected Layers: Combine features to form predictions. Architecture Example: A typical CNN might include: 1. Input layer (e.g., 224x224 RGB image) 2. Convolutional layer with 32 filters of size 3x3 followed by ReLU 3. Max pooling layer (2x2) 4. Convolutional layer with 64 filters followed by ReLU and another pooling layer 5. One or two fully connected layers 6. Output layer with softmax activation for classification. This layered approach allows CNNs to learn robust feature hierarchies that are highly effective in image recognition tasks.\",\\n      \"metadata\": {\\n        \"difficulty\": \"hard\",\\n        \"topics\": [\\n          \"CNN\",\\n          \"optical flow\"\\n        ]\\n      },\\n      \"evaluation-prompt\": \"Assess the depth of understanding of CNN architecture or optical flow principles, including mathematical accuracy for derivation.\",\\n      \"related_url\": []\\n    },\\n    {\\n      \"id\": \"Q8\",\\n      \"type\": \"either_or\",\\n      \"question_text\": \"Either (a) Explain in detail the different methods for object detection in images and videos. Compare traditional approaches with deep learning-based methods. Or (b) What is feature matching in computer vision? Discuss different feature matching techniques and their applications.\",\\n      \"diagrams\": [],\\n      \"marks\": 14,\\n      \"options\": [\\n        {\\n          \"option_id\": \"a\",\\n          \"text\": \"Explain in detail the different methods for object detection in images and videos. Compare traditional approaches with deep learning-based methods.\"\\n        },\\n        {\\n          \"option_id\": \"b\",\\n          \"text\": \"What is feature matching in computer vision? Discuss different feature matching techniques and their applications.\"\\n        }\\n      ],\\n      \"answer\": \"Object detection aims to locate and classify objects within images. Methods include: 1. Traditional Approaches: Sliding Window: Exhaustively search the image using windows at multiple scales. Feature-Based Methods: Use hand-crafted features such as SIFT or HOG combined with classifiers (e.g., SVM). Pros: Intuitive and interpretable; effective in controlled environments. Cons: Computationally intensive, less accurate with complex backgrounds, and limited by the quality of hand-crafted features. 2. Deep Learning-Based Methods: Two-Stage Detectors: Such as R-CNN, Fast R-CNN, and Faster R-CNN, which first generate region proposals and then classify them. One-Stage Detectors: Such as YOLO (You Only Look Once) and SSD (Single Shot MultiBox Detector) that perform detection in a single pass, balancing speed and accuracy. Pros: End-to-end learning from data, high accuracy, and robustness to variations in scale, illumination, and occlusion. Cons: Require large datasets and significant computational resources; often act as \\\\\"black boxes\\\\\" with less interpretability. Deep learning methods have largely overtaken traditional methods due to their ability to learn complex representations from data. However, the choice of method may depend on application constraints such as speed, accuracy, and resource availability.\",\\n      \"metadata\": {\\n        \"difficulty\": \"hard\",\\n        \"topics\": [\\n          \"object detection\",\\n          \"feature matching\"\\n        ]\\n      },\\n      \"evaluation-prompt\": \"Evaluate the comprehensive understanding of object detection methods or feature matching techniques, including comparisons and applications.\",\\n      \"related_url\": []\\n    }\\n  ],\\n  \"metadata\": {\\n    \"created_by\": \"Exam Committee\",\\n    \"version\": \"1.0\",\\n    \"rubric_exam_name\": {\\n      \"A\": {\\n        \"questions\": [\\n          \"Q1\",\\n          \"Q2\",\\n          \"Q3\",\\n          \"Q4\",\\n          \"Q5\"\\n        ],\\n        \"max_attempts\": 5,\\n        \"marks_per_question\": 2\\n      },\\n      \"B\": {\\n        \"questions\": [\\n          \"Q6\",\\n          \"Q7\"\\n        ],\\n        \"max_attempts\": 2,\\n        \"marks_per_question\": 13\\n      },\\n      \"C\": {\\n        \"questions\": [\\n          \"Q8\"\\n        ],\\n        \"max_attempts\": 1,\\n        \"marks_per_question\": 14\\n      }\\n    },\\n    \"total_points\": 50,\\n    \"pass_threshold\": 15\\n  }\\n}\\n```'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_key_path = \"ans_key.png\"\n",
    "\n",
    "extracted_text = ocr_answer_key_to_qp_json(answer_key_path, str(json_data))\n",
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_title': 'Comprehensive Exam Paper',\n",
       " 'subject': 'Computer Vision',\n",
       " 'subject-id': 'XXXX',\n",
       " 'exam_date': '[Month/Year]',\n",
       " 'duration': 120,\n",
       " 'instructions': 'Answer all questions as directed. Use diagrams where necessary and provide clear, concise answers.',\n",
       " 'questions': [{'id': 'Q1',\n",
       "   'type': 'short_answer',\n",
       "   'question_text': 'Define image convolution and its significance in computer vision.',\n",
       "   'diagrams': [],\n",
       "   'marks': 2,\n",
       "   'options': [],\n",
       "   'answer': 'Image convolution is a mathematical operation on two functions (in this case, the image and a kernel/filter) that produces a modified image emphasizing or suppressing certain features. It is used for smoothing, sharpening, edge detection, and feature extraction, making it fundamental for preprocessing and analyzing images in computer vision.',\n",
       "   'metadata': {'difficulty': 'easy',\n",
       "    'topics': ['image processing', 'convolution']},\n",
       "   'evaluation-prompt': 'Assess the correctness and completeness of the definition and explanation of significance.',\n",
       "   'related_url': []},\n",
       "  {'id': 'Q2',\n",
       "   'type': 'short_answer',\n",
       "   'question_text': 'What is the difference between edge detection and corner detection?',\n",
       "   'diagrams': [],\n",
       "   'marks': 2,\n",
       "   'options': [],\n",
       "   'answer': 'Edge detection identifies boundaries within images where there is a sharp change in intensity, typically indicating object borders. In contrast, corner detection finds points where edges meet or where there is a high curvature, which are useful as reliable feature points for tracking and matching.',\n",
       "   'metadata': {'difficulty': 'medium',\n",
       "    'topics': ['feature detection', 'edge detection', 'corner detection']},\n",
       "   'evaluation-prompt': 'Evaluate the clarity and accuracy of the distinctions drawn between edge and corner detection.',\n",
       "   'related_url': []},\n",
       "  {'id': 'Q3',\n",
       "   'type': 'short_answer',\n",
       "   'question_text': 'Explain the concept of homography in image transformations.',\n",
       "   'diagrams': [],\n",
       "   'marks': 2,\n",
       "   'options': [],\n",
       "   'answer': 'Homography is a transformation that maps points from one plane to another using a 3x3 matrix. It is used to correct perspective distortions, align images taken from different viewpoints, and is fundamental in tasks like image stitching and augmented reality.',\n",
       "   'metadata': {'difficulty': 'medium',\n",
       "    'topics': ['image transformation', 'homography']},\n",
       "   'evaluation-prompt': 'Assess the understanding of homography and its role in image transformations.',\n",
       "   'related_url': []},\n",
       "  {'id': 'Q4',\n",
       "   'type': 'short_answer',\n",
       "   'question_text': 'What are the challenges in object recognition using deep learning?',\n",
       "   'diagrams': [],\n",
       "   'marks': 2,\n",
       "   'options': [],\n",
       "   'answer': 'Challenges include: Data Requirements: Deep networks require large, diverse, and well-labeled datasets. Overfitting: With limited data, models may overfit and not generalize well. Computational Resources: Training deep networks is computationally intensive. Variability: Handling variations in lighting, occlusion, and pose can be difficult. Interpretability: Deep models are often seen as \"black boxes,\" making it hard to explain their decisions.',\n",
       "   'metadata': {'difficulty': 'hard',\n",
       "    'topics': ['object recognition', 'deep learning', 'challenges']},\n",
       "   'evaluation-prompt': 'Evaluate the identification and explanation of significant challenges.',\n",
       "   'related_url': []},\n",
       "  {'id': 'Q5',\n",
       "   'type': 'short_answer',\n",
       "   'question_text': 'Describe the role of feature extraction in computer vision.',\n",
       "   'diagrams': [],\n",
       "   'marks': 2,\n",
       "   'options': [],\n",
       "   'answer': 'Feature extraction involves identifying and isolating important pieces of information (features) from raw images such as edges, corners, textures, or shapes. These features form the basis for tasks like object recognition, image matching, and classification, reducing data complexity and improving processing efficiency.',\n",
       "   'metadata': {'difficulty': 'medium',\n",
       "    'topics': ['feature extraction', 'computer vision']},\n",
       "   'evaluation-prompt': \"Assess the understanding of feature extraction's importance in computer vision.\",\n",
       "   'related_url': []},\n",
       "  {'id': 'Q6',\n",
       "   'type': 'either_or',\n",
       "   'question_text': 'Either (a) Explain the various types of image segmentation techniques with examples. Or (b) Describe the working of the Harris Corner Detector with necessary mathematical derivations.',\n",
       "   'diagrams': [],\n",
       "   'marks': 13,\n",
       "   'options': [{'option_id': 'a',\n",
       "     'text': 'Explain the various types of image segmentation techniques with examples.'},\n",
       "    {'option_id': 'b',\n",
       "     'text': 'Describe the working of the Harris Corner Detector with necessary mathematical derivations.'}],\n",
       "   'answer': 'Image segmentation divides an image into meaningful regions. Techniques include: Thresholding: Separates regions by intensity values (e.g., separating foreground from background). Edge-based Segmentation: Detects object boundaries using gradient information. Region-based Segmentation: Groups pixels with similar properties, such as in region growing or splitting and merging. Clustering Methods: Algorithms like k-means cluster pixels based on color or texture similarities. Deep Learning Approaches: Methods such as Fully Convolutional Networks (FCN) and U-Net learn to segment images with high accuracy in complex scenes. In practice, each method is chosen based on the image characteristics and the specific application requirements.',\n",
       "   'metadata': {'difficulty': 'hard',\n",
       "    'topics': ['image segmentation', 'harris corner detection']},\n",
       "   'evaluation-prompt': 'Evaluate the completeness and accuracy of the explanation or derivation, including mathematical rigor where applicable.',\n",
       "   'related_url': []},\n",
       "  {'id': 'Q7',\n",
       "   'type': 'either_or',\n",
       "   'question_text': 'Either (a) Discuss the structure and working of a Convolutional Neural Network (CNN). Illustrate with a suitable architecture. Or (b) Explain the role of Optical Flow in motion estimation. Derive the Lucas-Kanade Optical Flow equation.',\n",
       "   'diagrams': [],\n",
       "   'marks': 13,\n",
       "   'options': [{'option_id': 'a',\n",
       "     'text': 'Discuss the structure and working of a Convolutional Neural Network (CNN). Illustrate with a suitable architecture.'},\n",
       "    {'option_id': 'b',\n",
       "     'text': 'Explain the role of Optical Flow in motion estimation. Derive the Lucas-Kanade Optical Flow equation.'}],\n",
       "   'answer': 'A Convolutional Neural Network (CNN) is composed of several types of layers designed to automatically and adaptively learn spatial hierarchies of features such as edges and textures. Convolutional Layers: Apply filters to extract features such as edges and textures. Activation Layers: Introduce non-linearity using functions like ReLU. Pooling Layers: Reduce the spatial dimensions, making the representation more compact and robust to slight translations (e.g., max pooling). Fully Connected Layers: Combine features to form predictions. Architecture Example: A typical CNN might include: 1. Input layer (e.g., 224x224 RGB image) 2. Convolutional layer with 32 filters of size 3x3 followed by ReLU 3. Max pooling layer (2x2) 4. Convolutional layer with 64 filters followed by ReLU and another pooling layer 5. One or two fully connected layers 6. Output layer with softmax activation for classification. This layered approach allows CNNs to learn robust feature hierarchies that are highly effective in image recognition tasks.',\n",
       "   'metadata': {'difficulty': 'hard', 'topics': ['CNN', 'optical flow']},\n",
       "   'evaluation-prompt': 'Assess the depth of understanding of CNN architecture or optical flow principles, including mathematical accuracy for derivation.',\n",
       "   'related_url': []},\n",
       "  {'id': 'Q8',\n",
       "   'type': 'either_or',\n",
       "   'question_text': 'Either (a) Explain in detail the different methods for object detection in images and videos. Compare traditional approaches with deep learning-based methods. Or (b) What is feature matching in computer vision? Discuss different feature matching techniques and their applications.',\n",
       "   'diagrams': [],\n",
       "   'marks': 14,\n",
       "   'options': [{'option_id': 'a',\n",
       "     'text': 'Explain in detail the different methods for object detection in images and videos. Compare traditional approaches with deep learning-based methods.'},\n",
       "    {'option_id': 'b',\n",
       "     'text': 'What is feature matching in computer vision? Discuss different feature matching techniques and their applications.'}],\n",
       "   'answer': 'Object detection aims to locate and classify objects within images. Methods include: 1. Traditional Approaches: Sliding Window: Exhaustively search the image using windows at multiple scales. Feature-Based Methods: Use hand-crafted features such as SIFT or HOG combined with classifiers (e.g., SVM). Pros: Intuitive and interpretable; effective in controlled environments. Cons: Computationally intensive, less accurate with complex backgrounds, and limited by the quality of hand-crafted features. 2. Deep Learning-Based Methods: Two-Stage Detectors: Such as R-CNN, Fast R-CNN, and Faster R-CNN, which first generate region proposals and then classify them. One-Stage Detectors: Such as YOLO (You Only Look Once) and SSD (Single Shot MultiBox Detector) that perform detection in a single pass, balancing speed and accuracy. Pros: End-to-end learning from data, high accuracy, and robustness to variations in scale, illumination, and occlusion. Cons: Require large datasets and significant computational resources; often act as \"black boxes\" with less interpretability. Deep learning methods have largely overtaken traditional methods due to their ability to learn complex representations from data. However, the choice of method may depend on application constraints such as speed, accuracy, and resource availability.',\n",
       "   'metadata': {'difficulty': 'hard',\n",
       "    'topics': ['object detection', 'feature matching']},\n",
       "   'evaluation-prompt': 'Evaluate the comprehensive understanding of object detection methods or feature matching techniques, including comparisons and applications.',\n",
       "   'related_url': []}],\n",
       " 'metadata': {'created_by': 'Exam Committee',\n",
       "  'version': '1.0',\n",
       "  'rubric_exam_name': {'A': {'questions': ['Q1', 'Q2', 'Q3', 'Q4', 'Q5'],\n",
       "    'max_attempts': 5,\n",
       "    'marks_per_question': 2},\n",
       "   'B': {'questions': ['Q6', 'Q7'],\n",
       "    'max_attempts': 2,\n",
       "    'marks_per_question': 13},\n",
       "   'C': {'questions': ['Q8'], 'max_attempts': 1, 'marks_per_question': 14}},\n",
       "  'total_points': 50,\n",
       "  'pass_threshold': 15}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = extract_json_from_string(extracted_text)\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_key_string = read_from_text(\"ans_key.txt\")\n",
    "\n",
    "extracted_text = answer_key_string_with_gemini_to_json(answer_key_string, str(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp_extracted_with_answers = extract_json_from_string(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Answer Sheet Extraction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_sheet_format = read_json_as_string(\"answer_sheet_pe.json\")\n",
    "\n",
    "answer_sheet_1 = extract_and_evaluate_answer_sheet_with_key(\"sample_answer_sheet.png\", qp_extracted_with_answers, answer_sheet_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_sheet_format': {'paper_title': 'Comprehensive Exam Paper',\n",
       "  'subject': 'Computer Vision',\n",
       "  'subject_id': 'XXXX',\n",
       "  'exam_date': '[Month/Year]',\n",
       "  'duration': '120',\n",
       "  'total_marks': 50,\n",
       "  'pass_threshold': 15,\n",
       "  'answers': [{'question_id': 'Q1',\n",
       "    'answer': 'Image convolution is a mathematical operation involving the multiplication of an image with a kernel (a small matrix).  The result is a modified image, highlighting or suppressing certain features. This is crucial for tasks like smoothing, sharpening, edge detection, and feature extraction – fundamental steps in image analysis and preprocessing within computer vision.',\n",
       "    'marks_obtained': 1,\n",
       "    'evaluation': 'The answer correctly defines convolution but lacks detail on the crucial steps such as flipping the kernel and summing the results, limiting its completeness.  The significance is mentioned but could be more elaborate.'},\n",
       "   {'question_id': 'Q2',\n",
       "    'answer': 'Edge detection pinpoints image boundaries with sharp intensity changes, often representing object borders.  Corner detection, on the other hand, focuses on points where edges meet or exhibit high curvature. These corner points are more reliable as features for tasks like matching and object tracking.',\n",
       "    'marks_obtained': 2,\n",
       "    'evaluation': 'The student successfully differentiates between edge and corner detection, accurately describing their purposes and applications.'},\n",
       "   {'question_id': 'Q3',\n",
       "    'answer': 'A homography is a projective transformation represented by a 3x3 matrix. It maps points from one plane to another, correcting perspective distortions and aligning images from varied viewpoints. This transformation is essential for applications like image stitching and augmented reality.',\n",
       "    'marks_obtained': 1,\n",
       "    'evaluation': 'The answer partially addresses homography by correctly mentioning the 3x3 matrix but fails to elaborate on its applications in detail.  The definition also could be clearer.'},\n",
       "   {'question_id': 'Q4',\n",
       "    'answer': 'Object recognition using deep learning faces several key challenges.  These include the need for extensive, diverse, and accurately labeled datasets.  The risk of overfitting, where a model performs well on training data but poorly on unseen data, is significant.  Training deep networks demands substantial computational resources, and handling variations in lighting, occlusion, and object pose remains a challenge. The inherent ‘black box’ nature of some deep learning models makes interpreting their predictions difficult.',\n",
       "    'marks_obtained': 2,\n",
       "    'evaluation': 'The answer correctly identifies multiple crucial challenges related to data, computational resources, and model interpretability. The points are well-explained.'},\n",
       "   {'question_id': 'Q5',\n",
       "    'answer': 'Feature extraction in computer vision involves the identification and isolation of significant information (features) from raw images. These features could include edges, corners, textures, or shapes.  Feature extraction simplifies the data, making it more manageable for tasks like object classification, image matching, and recognition, boosting processing efficiency.',\n",
       "    'marks_obtained': 1,\n",
       "    'evaluation': 'The answer is a good starting point but lacks detail on the different methods of feature extraction. Mentioning dimensionality reduction would have enhanced its quality.'},\n",
       "   {'question_id': 'Q6',\n",
       "    'answer': 'Image segmentation partitions an image into meaningful regions.  Several methods exist, including thresholding (separating regions based on intensity), edge-based segmentation (detecting boundaries using gradients), region-based segmentation (grouping pixels with similar properties), clustering (using algorithms like k-means), and deep learning approaches such as U-Net and FCN. The selection of the most appropriate technique depends on image characteristics and application requirements.',\n",
       "    'marks_obtained': 10,\n",
       "    'evaluation': 'The answer provides a comprehensive overview of image segmentation techniques, including various methods and their applications. The explanations are concise yet thorough.'},\n",
       "   {'question_id': 'Q7',\n",
       "    'answer': 'A Convolutional Neural Network (CNN) processes image data through a series of convolutional layers that use filters to extract features like edges and textures. Activation functions (e.g., ReLU) introduce non-linearity.  Pooling layers reduce spatial dimensions, improving robustness and efficiency. Fully connected layers combine features to produce predictions. A sample architecture would have an input layer, several convolutional and pooling layers, and a final fully connected layer for classification.',\n",
       "    'marks_obtained': 8,\n",
       "    'evaluation': 'While the answer describes the basic components of a CNN (convolutional, pooling, and fully connected layers), it lacks specific architectural details and the role of activation functions. Deeper explanations of each layer and the flow of data are missing.'},\n",
       "   {'question_id': 'Q8',\n",
       "    'answer': 'Object detection aims to identify and locate objects in images and videos. Traditional methods often involve sliding windows to scan the image at different scales and use hand-crafted features like SIFT or HOG with classifiers (e.g., SVMs). Deep learning methods like R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD use end-to-end learning, generally achieving better accuracy. Two-stage detectors, such as R-CNN variants, generate region proposals and then classify them. One-stage detectors like YOLO and SSD detect and classify directly in one step, enhancing speed. Deep learning methods have substantially surpassed traditional methods in accuracy and robustness but require large datasets and computational resources.  Traditional approaches are simpler but struggle with complex backgrounds and variations.',\n",
       "    'marks_obtained': 12,\n",
       "    'evaluation': 'The answer offers a good comparison of traditional and deep learning object detection methods. It effectively highlights strengths and weaknesses. However, a more detailed comparison of the specific architectures within each category would improve the answer.'}],\n",
       "  'total_marks_obtained': 44}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_json_from_string(answer_sheet_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AllPackages",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
